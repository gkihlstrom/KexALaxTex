
\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}

\begin{document}

\title{Technical analysis inspired machine learning for stock markets}
\author{Gustav Kihlström \& Patryk Przybysz}
\date{18/04/2016}
\maketitle
\newpage

\begin{abstract}
Abstraktkonst.
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction}
Forecasting the behavior of financial products is an important preoccupation for many professionals and laymen alike, as well as academics. Investment banks, hedge funds, and insurance companies spend significant time and effort in predicting changes in the properties, such as prices and risks, of stocks, bonds, options, etc. This interest in financial markets has for the last few decades driven the development of better tools for analyzing financial information. Most fruitful, historically, has perhaps the research in financial mathematics been. 

Technical analysis is the study of past information to predict future values. Technical analysis is commonly used on financial markets and can be used on for example stock prices. Mathematical concepts are used, as an example the average moving index, however the mathematical sophistication of these are generally low. Technical analysis also takes the form of grouping continuous data into discrete packets of data to visually summarize the data, sometimes referred to as candlesticks.

Candlestick charts are frequently used in technical analysis to represent time series. Time series are divided into specific units, most commonly days, and transformed into “candlesticks”. Each candlestick holds information about the starting and final value of that time period as well as the highest and lowest levels reached in between. A candlestick can thereby be summarized as start and end values combined with the volatility of the underlying value.

Financial mathematics is the use of advanced mathematical and statistical methods to price and minimize risk of financial products and portfolios. An example of this is Black-Scholes method for options pricing (Black, et al 1973) and the LIBOR market model for pricing interest rates derivatives (Bruce, et al. 1997). With the increasing access to computational power the use of artificial intelligence, particularly machine learning has become an important aspect of market prediction. 

Machine learning has been successfully used for time-series financial forecasting. Support vector machines (SVM) and artifical neural networks (ANN) have been widely used  within academics to predict financial data. Cao and Tays work on SVMs and ANNs cemented SVMs as an important tool for stock prediction. The SVM has since been expanded upon to form a multiple-kernel based learning system for stock prediction (Fletcher, 2012) (Yeh, et al., 2010) and credit risk evaluation (Yu, et a.l, 2009), and the extreme learning machines (ELM), a type of ANN, has been developed and used for financial prediction (Ding, et al 2015). Simpler approches such as desicion trees have also been successfully used (Audrino, et al 2010), whereas there's very little research done on the Naive-Bayes classifier for stock data.

There has been little research on the performance of various combinations of machine learning and technical analysis methods. The research that has been made on the subject is generally focused on one, or a few, lerning algorithms and little weight is put on the the importance of the choice of parameters. As such there is a gap of knowledge as to which parameters work well with which algorithms. It is our ambition to contribute to filling that gap by analysing the performance of a few selected algorithms from different machine learning paradigms with alternating input parameters.

\subsection{Problem statement}
What is the performance of the SVM, ELM, Random-Forest, and Naive-bayes algorithms on financial data with respect to changes in price, volatility, trading volume, and different time intervals, as well as combinations of these?


\section{Background}
\subsection{Technical analysis}
Technical analysis is a method for anticipating market price changes by studying historic data for that market. It primarily focuses on past price levels, but trading volumes are used as well. The fundamental assumption of technical analysis is that market prices reflect all available information (Snopek, 2012) separating it from other security analysis methodologies such as fundamental analysis, where the state of the underlying asset is ascertained by reviewing its financial statements. This reflection is not seen as perfect because the relevant information might not always be wholly available due to, for example, insider trading (Snopek, 2012). 
\\
According to technical analysis, prices move in distinct trends. A trend is bullish if the highest and lowest price levels for a security are gradually higher and bearish if the opposite is true. A flat trend occurs when price levels stay the same during a longer time interval.
\\
A third concept of technical analysis is that history repeats itself and "the future is just a repetition of the past" [inre källa från källan] and so past errors and events, such as market crashes or speculative bubbles, might be repeated by new generations.
\\
There is no consensus whether technical analysis can be effectively used as a investment strategy. Studies have been made that show technical analysis to be no better than simply buying a security and holding it until an opportune time, without any portfolio strategy.

\subsubsection{Volatility}
Volatility has been widely used in financial market prediction. Many models within mathematical finance, including the Black-Scholes model, use volatility as one of the main components for handeling pricing. Machine learning algorithms have been used to predict future volatility, including SVMs (Wang, et al. 2011), ELMs (Wang, et al. 2014), and regression trees (Audrino, et al. 2009).  However little research on the predictive capabilities of using volatility as an indicator for future market behavior has been done. In this paper we will examine how using volatility as a predictor, on several different machine learning algorithms, applied on stock price data influences the accuracy in prediction of future price developement. 

\subsubsection{Volume}
The volume of trades, i.e. the number of trades done during the time interval of some underlying asset, is one of the main aspects of technical analysis. The general idea is that changes in price of the asset together with a small volume will have a different impact on the market than the same change in price associated with a larger trading volume. 

\subsection{SVM}
The SVM algorithm is effectively trying to place a high-dimensional plane, or hyperplane, between the differently classified sets of datapoints and predicting new datapoints based on which side of the hyperplane they are located. Data is however often not dividable by a plane, which the SVM handles using slack variables and kernels. \\
The aim of a basic seperating hyperplane algorithm is to seperate the different classes of data using a hyperplane, while achieving the maximum margin between the closest data points of the different classes and the seperating plane. SVM is an extension of the seperating hyperplane idea by representing the data in higher dimensions to spread the distrinution of the low dimensional data. This can be summarized as the following optimization problem;
\begin{equation}
\min \frac{1}{2}\vec{w}^{T}\vec{w}
\end{equation} 
\begin{center}
 subject to  $t_i \vec{w}^{T} \Phi(\vec{x}_i)  \geq 1$,  \\
\end{center}
where $\vec{w}$ is the weight vector of the hyperplane, $\vec{x}_i$ is the $ith$ training point, $t_i$ is the class of the $ith$ training point, and $\Phi(\vec{x})$ is the non-linear transformation used. This formulation is however often too rigid, and unable to find an optimal solution if there are data points from different classes intertwined. To deal with this problem slack variables are introduced and the problem is rewritten as; 
\begin{equation}
\min \frac{1}{2}\vec{w}^{T}\vec{w} + C\sum\limits_i \zeta_i 
\end{equation}
\begin{center}
subject to $t_i \vec{w}^{T} \Phi(\vec{x}_i)  \geq 1 - \zeta_i, $
\end{center}
where $\zeta_i$ are the slack variables and $C$ is a slack constant (James, G. et al, 2014). Also this formulation can be restated, in the form of it's dual problem;

\begin{equation}
\max \sum\limits_i \alpha_i - \frac{1}{2}\sum\limits_{i,j} \alpha_i \alpha_j t_i t_j \kappa (\vec{x_i}, \vec{x_j}) 
\end{equation}
\begin{center}
subject to $0 \leq \alpha_i \leq C, $
\end{center}
where $\kappa (\vec{x_i}, \vec{x_j}) = \Phi(\vec{x_i})^T \Phi(\vec{x_j})$ is the kernel. Once the optimization problem has been solved, i.e. the model has been trained, unseen data points, $\vec{x}$,  are classified with $\sum\limits_i \alpha_i t_i \kappa(\vec{x},\vec{x_i}) > 0$. 


\subsection{Random Forest}
Random forest is a classification algorithm belonging to the tree-based machine learning algorithms. Tree-based methods attempt to divide the space of possible outcomes into generally simple areas. This is done by splitting the outcome space in two, one feature at a time. Tree-based methods are believed to reflect the way humans make decisions. They also allow for easy inference even in its simplest forms, but usually need to be improved in order to reach prediction accuracy rates comparable to other algorithms (James, G. et al, 2014). Such improvement can be attained by implementing Random Forest.

\subsubsection{Decision trees}
KANSKE MÅSTE FÖRENKLA LITE HÄR
The decision tree classifier (also a regressor) constructs a logical sqeuent, in the form of a tree data structure. The idea is to identify the attribute, at the current node in the tree, on which one can gain the most information from splitting on. This is done by comparing the entropy of the data set before and after the split, i.e. the information gain of the split;
\begin{equation}
I(S, A) = Ent(S) - \sum\limits_{v \in Values(A)} \frac{|S_v|}{|S|} Ent(S_v),
\end{equation}
where $S$ is the training set, $A$ is a attribute of the training set, and $S_v$ is the set after the split containing the value $v$ on attribute $A$. $Ent(S)$ is the entropy of the set S. Once the tree has been trained the sequent formed is used to classify unseen data points.
\subsubsection{Bagging}
Decision trees can attain very different results even if they are trained on the same dataset making them unpredictable. [BOK] Bootstrap aggregation, bagging in short, is an attempt to make decision trees more predictable when trained on the same dataset in order to achieve consistent prediction accuracy rates. \\ \\
Bagging uses bootstrapped replicates of the training set to train several instances of and combine a simple classifier, $f_b$ to form a more powerful predictor, $f_{bag}$. This is done by creating $B$ bootstrapped training sets, $S_b$, by sampling with replacement from the given training set $S = \{ (\vec{x}_1, y_1), ..., (\vec{x}_m, y_m)\}$, where possibly $|S_b| > m$. The classifier $f_b$ is then trained using $S_b$, and unseen data points are classified using; 
\begin{equation}
f_{bag}(\vec{x}) = \max\limits_k \sum\limits_b Ind(f_b(\vec{x}) = k),
\end{equation}
where $k$ is the class, and $Ind(a = b) = 1$ if $a = b$ and $Ind(a = b) = 0$ if $a \ne b$.

\subsubsection{Random forest}
The random forest method is similar to bagged trees, it uses bootstrapping to construct several training sets and subsequently trains decision trees on these to create a classifier with greater accuracy than the individual decision trees. It differs from bagging in the way the predictors are used in splitting. Whenever a split in a tree is considered, only a random subset $n$ of the available predictors are provided as possible candidates for splitting (James, G. et al, 2014). This is done to prevent the algorithm from creating very similar trees due to one particularly strong predictor always being chosen in the splitting process.
 

\subsection{Extreme learning machines}
The extreme learning machine is a type of ANN, namely a single layer feedforward network (SLFN). This means that the algorithm takes input and pushes it forward to hidden nodes, who in turn outputs a classification for new datapoints. The standard mathematical representation of an SLFN is;
\begin{equation}
\sum\limits_{i = 1}^N \vec{\beta}_i f_i (\vec{x}_j)  = \sum\limits_{i = 1}^N \vec{\beta}_i f_i (\vec{a}_i  \vec{x}_j + b_i)= t_j, j = 1...n, 
\end{equation}
where $N$ is the number of nodes in the hidden layer, $\vec{a}_i$ is the is the input-weight vector for the $i$th node, $\vec{\beta}_i$ is the output-weight vector for the $i$th node, $b_i$ is the threshold of the $i$th node, and $f_i(\vec{x})$ is the activation function (or kernel in SVM terminology) (Liu, X, et al. 2013). This can in turn be written comapctly as $H\beta = T$, where $H$ is an $n \times N$ matrix and $H_{i, j} = f(\vec{a}_j\vec{x}_i + b_j)$, $\beta$ is a vector with $\beta_i^T$ in position $i$, and $T$ is a vector with the sample outputs. To train the classifier one initiates all $\vec{a}_i$ and $b_i$ to random values, and compute the output-weight vector $\hat{\beta}$ from $\hat\beta = H^+ T$, where $H^+$ is the Moore-Penrose pseudo-inverse of $H$ (Li, X. et al 2016).

\subsection{Naive Bayes}
The naive bayes classifier is a probability based classifier based on Bayes' Theorem. The main idea is to, instead of fitting $k$-dimensional data in to one probability distribution, to fit the data into $k$ separate, one-dimensional  distributions. In order to use the algorithm, a critical assumption has to be made, namely that the features are regarded as independent. This assumption is what makes this algorothm 'naive'. Using the maximum a priori method (MAP), the naive bayes can be implemented as follows:
\begin{center}
$Y_{MAP} = \arg \max_{y \in Y} P(y | x_1, ..., x_k) = arg \max_{y \in Y} \frac{ P( x_1, ..., x_k | y)P(y) }{P(x_1, ..., x_k)}=$
\end{center}
\begin{equation}
 = arg \max_{y \in Y} P(x_1, ..., x_k|y)P(y) ,
\end{equation}
where $\vec{x} = (x_1, ..., x_k)$ is a $k$-dimensional data point, and $Y = \{1, 2, ...,y_n\}$ are the possible classes. Since the features are regarded as independent we are assuming that $P(x_1, ..., x_k| y) = \prod_{k=1}^{K} P(x_k |y).$ As such the classifier becomes $Y_{MAP} = arg \max_{y \in Y} P(y)\prod_{k=1}^{K}P(x_k|y)$ (James, G. et al, 2014).

\subsection{Cross-validation}
Cross-validation is used to ensure that the prediction accuracy of a learning algorithm is properly estimated. The data set is split into $p$ partitions and the algorithm is trained on$p-1$ of the subsets and then the predictive accuracy of the algorithm is tested on the remaining subset. This process is repeated $p$ times, called $p$-fold cross-validation, using every subset as the testing set once. The performance of the different tests are then averaged.

\newpage

\section{Method}
The aim of this paper is to evaluate the influence of using a volatility and volume measure as an indicator of future stock price developements. This will be done by running the different algorithms on sample data sets using firstly only past end-of-day stock prices for different number of days looking in the past. Then it will be also be run on price and volatility measure combined, price and volume combined, and lastly price, volatility and volume measures combined. 

\subsection{Price}
The price used will be normalized, i.e. every end-of-day stock price will in fact be a percentage of the start-of-day price.

\subsection{Volatility measure}
Due to the lack of precision data the volatility will have to be approximated. In this paper we will approximate the volatility by letting the highest and lowest prices of the trading day be parameters of the data used to train the machine learning algorithms. As with the price we also here use the percentage of the start-of-day price.

\subsection{Volume}
The volume parameter is also normalized. This is done by taking the average trading volume over the entire time interval and dividing each individual volume data point by this number. 

\subsection{Days looking backwards}
The algorithms are run on 5 data input using five different time aspects. These are looking backwards on the 2, 3, 5, 10, and 30 previous trading days. The various combinations mentioned above are implemented on each of theprevious days. This gives, as an example, nine features per data point for the algorithms running price and volatility (highest and lowest points) looking backwards three days.

\subsection{Data set}
The algoithms will be run on the S\&P500 index. The data set contains daily trading information for the stocks from January 3 2000 to 27 March 2016, including opening and closing prices, highest and lowest prices, and dividend adjusted price change. 

\subsection{Accuracy} 
To evaluate the performance of the algorithms we look at the prediction accuracy. Two different approaches to determining the significance ofthe accuracy are used. The ELM algorithm is run ten times on the different data sets and an average of the prediction accuracy is taken to ensure a representative value. On the other three algorithms ten-fold cross-validation is used. 

\subsection{Algorithmic specifications}
For both ELM and SVM the radial basis kernel, $\Phi(\vec{x}) = e^{-\frac{(\vec{x} - \vec{k})^2}{r^2}}$, is used. For ELM 100 hidden nodes are used. For Random Forest 100 trees are trained with unlimited depths.

\newpage

\section{Experimental results}
\subsection{SVM}
The following tables show the accuracy of the SVM using the different combinations of input features. The accuracy is given in percentages.
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 50.9679  \\ \hline
    3 & 51.0539  \\ \hline
    5 & 51.4468  \\ \hline
    10 & 51.3135  \\ \hline
    30 & 51.6408 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.2129  \\ \hline
    3 & 51.6176  \\ \hline
    5 & 51.2997  \\ \hline
    10 & 51.3135  \\ \hline
    30 & 51.3694 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.1149  \\ \hline
    3 & 51.4706  \\ \hline
    5 & 51.2016  \\ \hline
    10 & 50.9452  \\ \hline
    30 & 51.3447 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.2374  \\ \hline
    3 & 51.5441  \\ \hline
    5 & 51.3242  \\ \hline
    10 & 50.9944  \\ \hline
    30 & 51.3694 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}

\includegraphics[width=1.25\textwidth, center]{svm_graph.png}

As seen, all configurations aside from price only lead to similar prediction accuracy rates. Accuracy rates range approximately between $51\%$ and  $51.7\%$. The highest score was attained by the 30-day price only configuration, followed by the 3-day price and volatility set. The price only configuration is the only one that appears to have more accurate results with the increase of lookback [ÄNDRA KANSKE] days.
\newpage
\subsection{ELM}
The following tables show the accuracy of the ELM using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 &  51.7 \\ \hline
    3 & 53.05  \\ \hline
    5 & 53.26  \\ \hline
    10 & 51.91  \\ \hline
    30 & 53.1 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 54.15  \\ \hline
    3 & 51.9  \\ \hline
    5 & 51.63  \\ \hline
    10 & 51.33  \\ \hline
    30 & 52.61 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.37  \\ \hline
    3 & 51.63  \\ \hline
    5 & 50.66  \\ \hline
    10 & 51.33  \\ \hline
    30 & 52.3 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.54  \\ \hline
    3 & 53.61  \\ \hline
    5 & 52.24  \\ \hline
    10 & 52.91  \\ \hline
    30 & 50.96 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}
\includegraphics[width=1.25\textwidth, center]{elm_graph.png}

Here, no one configuration of features consistently outperforms any of the others. Accuracy rates range approximately between $51\%$ and $54\%$. The highest rate is achieved by the 2-day price and volatility set but has lower rates than both the price only and the complete configuration when the number of lookback days  increases.

\newpage
\subsection{Random Forest}
The following tables show the accuracy of the Random-Forest algorithm using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.37  \\ \hline
    3 & 50.01  \\ \hline
    5 & 50.49  \\ \hline
    10 & 50.87  \\ \hline
    30 & 52.15 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.07  \\ \hline
    3 & 50.72  \\ \hline
    5 & 51.21  \\ \hline
    10 & 52.17  \\ \hline
    30 & 52.02 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.47  \\ \hline
    3 & 52.75  \\ \hline
    5 & 51.78  \\ \hline
    10 & 51.38  \\ \hline
    30 & 51.7 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.81  \\ \hline
    3 & 51.68  \\ \hline
    5 & 51.34  \\ \hline
    10 & 52.14  \\ \hline
    30 & 52.32 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}
\includegraphics[width=1.25\textwidth, center]{random_forest_graph.png}

Accuracy rates range approximately between $50\%$ and $53\%$. All configurations except price and volume follow a similar pattern. The 3-day price and volume achieves the best accuracy, followed by the 30-day price volatility and volume configuration.

\newpage
\subsection{Naive-Bayes}
The following tables show the accuracy of the Naive-Bayes algorithm using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.00  \\ \hline
    3 & 52.73  \\ \hline
    5 & 52.78  \\ \hline
    10 & 53.13  \\ \hline
    30 & 52.42 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.50  \\ \hline
    3 & 52.56  \\ \hline
    5 & 52.35  \\ \hline
    10 & 52.12  \\ \hline
    30 & 52.14 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.66  \\ \hline
    3 & 52.30  \\ \hline
    5 & 51.80  \\ \hline
    10 & 51.74  \\ \hline
    30 & 52.13 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.01  \\ \hline
    3 & 52.42  \\ \hline
    5 & 51.93  \\ \hline
    10 & 52.29  \\ \hline
    30 & 52.69 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}

\includegraphics[width=1.25\textwidth, center]{bayes_graph.png}

Accuracy rates range approximately between $51.5\%$ and $53\%$. The 10-day price only configuration achieves the best accuracy, followed by a tie between the 2-day price only configuration and the 2-day complete configuration.

\subsection{All algorithms}

\includegraphics[width=1.25\textwidth, center]{all_algorithms.png}
Accuracy ranges approximately from $51.5\%$ to $52.5\%$, making the gap narrow, compared to the individual results.

\subsection{All configurations}

\includegraphics[width=1.25\textwidth, center]{all_configs.png}
Accuracy ranges from $51.2\% - 53\%$ at 2 days and the gap continually decreases as more days are added. At 30 days the accuracy gap has closed in to a range from $51.5\% - 52.5\%$.

\newpage

\section{Discussion}
\subsection{SVM}
There are trends in the data. All features combinations perform better for 3 days looking backwards than 2 days. The price only measure has a general upwards curve of prediction accuracy with more days looking backwards. This indicates that 2 days is suboptimal for predcition. The price only measure is outperformed by all other measures on 2 and 3 days, but outperformes all other measures for 5 days and more, with the exception of price and volatility for 10 days. This indicates that the additional information is useful on a smaller timescale, but impairs prediction accuracy on longer ones by creating unnecessary noise. The exception, as mentioned, being price and volatility on 10 days. This, together with the fact that price and volatility constantly outperforms the volume included measures, suggests that the volatility measure is more reliable than the volume measure. However, since the top performing measure is the price only for 30 days implies that volatility only is reliable for shorter time frames.
\\ \\
It is worth noting that the accuracy rates fit in narrow range between $51\%-51.7\%$. This makes it uncertain whether the different configurations have a significant influence on the learning algorithm. Furthermore, the SVM achieved the lowest accuracy rates out of all tested methods.
\\ \\
The given results from the SVM can be seen as reliable, due to the use of cross-validation in all configurations.

\subsection{ELM}
The performance of the price and volatility and price and volume measures have similar developments over difference in days, with the price and volatility measure constantly performing better or on par with price and volume. This indicates that volatility is a better feature for indicating stock behaviour. However, the price only measure in turn outperforms both of these for 3 and more days. The price, volatility and volume measure performs at or around the best measure for all days except 30. This decline in accuracy could be caused by the large number of input parameters looking back 30 days. Should this be the case, it would seem that the ELM algorithm is able to use both volatility and volume together to create a measure better than the individual measures. 
\\ \\
The accuracy has a wider range than that of the SVM discussed above, with around $3$ percentage points between the best and worst performer. The ELM also receives some of the higher accuracy rates out of all algorithms.
\\ \\
Due to the implementation limitations of the used ELM algorithm, cross-validation was not used. Instead, reliability was calculated by using a part of the dataset as a testing set. Although, with enough iterations, the results can be considered reliable, it would have been better to assure training consistency with cross-validation. 

\subsection{Random Forest}
All measures perform comparably for 2 and 30 days. The price only measure is outperformed on all timescales in between, which implies that the addition of information is beneficial for the predictive accuracy of the random forest algorithm. The addition of volume gives rise to siginificant rise in accuracy for 3 days and also outperforms all other measures for 5 days. The volatility, however,  measure performs better for 10 and 30 days. The combined volatility and volume measure performs better or on par with the volatility measure for all timeframes. This signals that the volume measure should be used for all number of days, with the addition of the volatility measure when using more than 5 days of looking backwards, to create the optimal random forest stock predictor.
\\ \\
These differences in configuration can be seen as potentially significant due to the reasonably wide range of accuracy rates of about $2.5$ percentage points, close to the range of the ELM.
\\ \\
Training results have been made consistent by applying 10-fold cross-validation and can thus be seen as reliable.

\subsection{Naive Bayes}
The price only measure performs better, or marginally worse, than all other combinations of measure for all days, with a sharp decline in accuracy from 10 to 30 days. This suggests that the addition of features impairs the performance of the naive bayes classifier, for stock data. The volume configuration, despite receiving low accuracy rates for 5 and 10 days, seems to improve for the 30-day period. This could explain the strong result for the 30-day combined price, volatility and volume configuration, possibly indicating that volume is useful when observing long time periods. There is significant difference in the performance of the volatility and volume measures, compared to the price only measure. However as the accuracy of the price only configuration drops at the 30-day mark, the combined price, volatility and volume measure as well as just price and volume increases. This could indicate that more data features than just price may increase accuracy for longer timescales but are less beneficial when only looking back a few number of days. 
\\ \\
The accuracy range is almost exactly the same for the Naive Bayes Classifier as it is for the Random Forest algorithm, but reaches slightly higher rates for the price only configuration.
\\ \\
Same as previously, the reliability of the Naive Bayes Classifier is safeguarded by 10-fold cross-validation. 

\subsection{All algorithms}
The price only configuration seems to be have improved accuracy rates with the increase of lookback days. Price and volatility together with the complete price, volatility and volume configurations attain the highest accuracy rates at the 2-day mark and then gradually worsen. The price and volatility as well as price and volume experience a slight recovery at 30 days but no similar effect in the complete configuration is seen. Price and volume appears to have the lowest consistent accuracy, being surpassed for all days by the complete configuration, except for 30 days where the two sets are reach very similar results.

\subsection{All configurations}
At the 2 days of lookback mark there are two clear groups, the ELM and Naive-Bayes significantly outperforming the Random Forest and SVM. As more days are added, the performance of Naive-Bayes and ELM decrease and the Random Forest performance increases, leaving all three on a comparable level. The SVM, however, only make negligible changes in performance. This indicates that Random Forest is better suited for longer timescales, whereas ELM and Naive-Bayes are better suited for shorter ones, and SVM performance is not significantly affected by addition of past data. 

\section{Conclusions}
It is difficult to draw any general conclusions that apply to all algorithms, instead we can conclude that the optimal combinations of features and days of the algorithms vary. For the SVM using additional measure, as opposed to only price data, is beneficial for short time frames. When using additional features, volatility should be used. However, as more days are used this addition of data will impair predictive performance. As with the SVM, the volatility measure outperforms volume for the ELM. Combing the two does seem to give a boost in performance for days 3 to 10. Unlike the SVM, for the ELM there are no trends in optimal performace over time. However, using the price and volatility measure for 2 days lookback outperforms all othe combinations and algorithms. For Random Forest the addition of feature parameters increases performance significantly. The volume measure perfroms well for shorter time frames, and the volatility for longerones. The volatility and volume measure is carried for days 3 and 5 by the volume measure and for days 10 and 30 by the volatility measure, to create a stable predictor over all time frames. As opposed to the Random Forest algorithm, the Naive-Bayes performs worse in general when additional features are added to the price only measure. In tweaking prediction performance the number of lookback days is fundamental for all algorithms except SVM. The usage of different features to boost performance is also highly dependet on the number of days. In general the volatility measure performs better than the volume measure, and they are both outperformed by the combined volatility volume measure.  However, in some cases the price only measure outperforms all three other by a significant margin. Based on this we draw the conclusion that what feature, number of lookback days, that performs best is highly dependent on the algorithm. However, when designing machine learning algorithms, it would be wise to consider the volatility measure. 


\newpage
\section{Sources}
Liu, X., Wang, l., Huang, G., Zhang, J., Yin, J. (2013). Multiple kernel extreme learning machine. 149 Neurocomputing, 253-264. \\
Wang, F., Zhao, Z., Li, X. Yu, F., Zhang, H. (2014), Stock Volatility Prediction using Multi-Kernel Learning based Extreme Learning Machine. Beijing: 2014 International Joint Conference on Neural Networks, (SIDOR?). \\
Yeh, C., Huang, C., Lee, S. A multuple-kernel support vector regression approach for stock market price forecasting. (2011), 38 Expert Systems with Applications, 2177-2186. \\
James, G., Witten, D., Hastie, T., Tibshirani, R. An introduction to statistical learning [BOK, HUR?!] \\
Wang, B., Huang, H., Wang, X. A support vector machine based MSN model for financial short-term volatility prediction. (2013), 22 Nueral Computing \& Applications, 21-28. \\
Ding, S., Zhao, H., Zhang, Y., Xu, X., Nie, R. Extreme learninh machine: algorithms, theory and applications. (2013), 44 Artificial Intelligence Review, 103-115. \\
Audrino, F., Colangelo, D. Semi-parametric forecasts of the implied volatility surface using regression trees. (2009), 20 Statistical Computing, 421-434. \\
Black, F., Shcoles, M. The Pricing of Options and Corporate Liabilities. (1973), 81 Journal of Political Economy, 637-654. \\
Bruce, A. The Market Model of Interest Rate Dynamics. (1997), 7 Mathematical Finance, 127-147. \\
Li, X., Xie, H., Wang, R., Cai, Y., Coa, J., Wang, F., Min, H., Deng, X. Empirical analysis: stock market prediction via extreme learning machine. (2013), 27 Neural Computing \& Applications, 67-78. \\
Cao, L., Tay, F. Financial Forecasting using Support Vector Machines. (2001), 10 Neural Computing \& Applications 184-192. \\
Fletcher, T., Shawe-Taylor, J. Multiple Kernel Learning with Fisher Kernels for High-Frequency Currency Prediction. (2013), 42 Computational Economics, 217-240. \\
Yu, L., Yue, W., Wang, S., Lai, K. Support vector machine based multiagent ensemble learning for credit risk evaluation. (2010), 37 Expert Systems \& Applications 1351-1360. \\
Snopek, L. The Complete Guide to Portfolio Construction and Management. (2012) [SEN?] \\

\end{document}